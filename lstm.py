# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16RScKClafoPd_4YRQK4iRTRIUDpmsFQa
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding,LSTM, Dense,Dropout
from sklearn.preprocessing import LabelEncoder
import csv

imdb = pd.read_csv('IMDB Dataset.csv',engine="python",error_bad_lines=False)
imdb.head()

imdb.sentiment.value_counts()

text=imdb['review'][10]
print(text)
print("---------------------")
print(word_tokenize(text))

corpus=[]
for text in imdb['review']:
  words=[word.lower() for word in word_tokenize(text)]
  corpus.append(words)

num_words=len(corpus)
print(num_words)

imdb.shape

train_size=int(imdb.shape[0]*0.8)
X_train=imdb.review[:train_size]
y_train=imdb.sentiment[:train_size]

X_test=imdb.review[train_size:]
y_test=imdb.sentiment[train_size:]

tokenizer=Tokenizer(num_words)
tokenizer.fit_on_texts(X_train)
X_train=tokenizer.texts_to_sequences(X_train)
X_train=pad_sequences(X_train,maxlen=128, truncating='post',padding='post')

X_train[0],len(X_train[0])

X_test=tokenizer.texts_to_sequences(X_test)
X_test=pad_sequences(X_test,maxlen=128,truncating='post',padding='post')

X_test[0],len(X_test[0])

print(X_train.shape,y_train.shape)
print(X_test.shape,y_test.shape)

le=LabelEncoder()
y_train=le.fit_transform(y_train)
y_test=le.transform(y_test)

model=Sequential()
model.add(Embedding(input_dim=num_words,output_dim=100,input_length=128,trainable=True))
model.add(LSTM(100,dropout=0.1,return_sequences=True))
model.add(LSTM(100,dropout=0.1))
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

model.summary()

history=model.fit(X_train,y_train,epochs=5,batch_size=64,validation_data=(X_test,y_test))

plt.figure(figsize=(16,5))
epochs=range(1,len(history.history['accuracy'])+1)
plt.plot(epochs,history.history['loss'],'b',label='Training loss')
plt.plot(epochs,history.history['val_loss'],'b',label='Validation loss')
plt.legend()
plt.show()

#validation_sentence=['this movie was not good at all. it had some good parts like the acting was pretty good but story was not impressive at all']
validation_sentence=['i can watch the movie forever just because of beuty of cinematography']
validation_sentence_tokened=tokenizer.texts_to_sequences(validation_sentence)
val_sent_pad=pad_sequences(validation_sentence_tokened,maxlen=128,truncating='post',padding='post')
print(validation_sentence[0])
print(model.predict(val_sent_pad)[0])